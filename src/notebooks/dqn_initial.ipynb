{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, deque\n",
    "from gym import Env, spaces\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.losses import Huber\n",
    "from keras.initializers import Zeros\n",
    "from keras.optimizers import adam_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.valid_words import words\n",
    "\n",
    "WORDS = words\n",
    "WORDS_N = len(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ALPHA = 0.002\n",
    "EPSILON = 0.9\n",
    "EPSILON_DECAY = 0.99995\n",
    "EPSILON_MIN = 0.1\n",
    "GAMMA = 0.95\n",
    "\n",
    "# DQN Configuration\n",
    "BATCH_SIZE = 32\n",
    "MAX_MEMORY = 2000\n",
    "\n",
    "# Agent\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        # Wordle environment\n",
    "        self.env = env\n",
    "        # Wordle observation space dimensions\n",
    "        self.dimensions = env.observation_space.shape\n",
    "        # Hyperparameters\n",
    "        self.alpha = ALPHA\n",
    "        self.epsilon = EPSILON\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "        self.epsilon_min = EPSILON_MIN\n",
    "        self.gamma = GAMMA\n",
    "        # Batch replay size\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        # Batch replay memory\n",
    "        self.memory = deque(maxlen=MAX_MEMORY)\n",
    "        # Training model\n",
    "        self.q = self.get_model()\n",
    "        # Prediction model\n",
    "        self.target_q = self.get_model()\n",
    "    # This returns a neural network model.\n",
    "    def get_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(512, input_dim=self.dimensions[1], activation=\"relu\", bias_initializer=Zeros()))\n",
    "        model.add(Dense(512, activation=\"relu\", bias_initializer=Zeros()))\n",
    "        model.add(Dense(self.env.action_space.n, activation=\"linear\"))\n",
    "        model.compile(loss=Huber(), optimizer=adam_v2.Adam(learning_rate=self.alpha))\n",
    "        return model\n",
    "    # This returns a random action with epsilon probability.\n",
    "    def get_epsilon_action(self, state):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        return self.env.action_space.sample() if np.random.random() < self.epsilon else np.argmax(self.target_q.predict(state.reshape(self.dimensions), verbose=0)[0])\n",
    "    # This records an experience.\n",
    "    def remember(self, state, action, reward, state_, done):\n",
    "        self.memory.append((state, action, reward, state_, done))\n",
    "    # This copies the weights from the trained model to the prediction model.\n",
    "    def target_train(self):\n",
    "        weights = self.q.get_weights()\n",
    "        self.target_q.set_weights(weights[:len(self.target_q.get_weights())])\n",
    "    # This fits the training model according to a random subset of experience.\n",
    "    def replay(self):\n",
    "        if len(self.memory) >= self.batch_size:\n",
    "            samples = random.sample(self.memory, self.batch_size)\n",
    "            x = []\n",
    "            y = []\n",
    "            for sample in samples:\n",
    "                state, action, reward, state_, done = sample\n",
    "                target = self.target_q.predict(state.reshape(self.dimensions), verbose=0)\n",
    "                if done:\n",
    "                    target[0][action] = reward\n",
    "                else:\n",
    "                    target[0][action] = reward + self.gamma * max(self.target_q.predict(state_.reshape(self.dimensions), verbose=0)[0])\n",
    "                x.append(state)\n",
    "                y.append(target)\n",
    "            self.q.fit(np.array(x), np.array(y), batch_size=self.batch_size, verbose=0)\n",
    "    # This is the main training loop in which the agent plays multiple games of Wordle.\n",
    "    def train(self, steps):\n",
    "        self.env.set_explore()\n",
    "        for i in tqdm(range(steps)):\n",
    "            rewards = 0\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.get_epsilon_action(state)\n",
    "                state_, reward, done, info = self.env.step(action)\n",
    "                self.remember(state, action, reward, state_, done)\n",
    "                # The prediction model is trained every 100 steps.\n",
    "                if i % 100 == 0:\n",
    "                    self.target_train()\n",
    "                # The agent replays memory every 4 steps.\n",
    "                if i % 4 == 0:\n",
    "                    self.replay()\n",
    "                rewards += reward\n",
    "                state = state_\n",
    "            if steps % 1000 == 0:\n",
    "                self.save(\"q\", \"target_q\")\n",
    "    # This returns the optimal action.\n",
    "    def get_optimal_action(self, state):\n",
    "        return np.argmax(self.target_q.predict(state.reshape(self.dimensions), verbose=0)[0])\n",
    "    # This saves the weights of the neural networks.\n",
    "    def save(self, f, target_f):\n",
    "        self.q.save_weights(f)\n",
    "        self.target_q.save_weights(target_f)\n",
    "    # This loads the weights of the neural networks from a file.\n",
    "    def load(self, f, target_f):\n",
    "        self.q.load_weights(f)\n",
    "        self.target_q.load_weights(target_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "class Wordle(Env):\n",
    "    def __init__(self):\n",
    "        self.words = WORDS\n",
    "        self.action_space = spaces.Discrete(len(WORDS))\n",
    "        self.observation_space = spaces.Box(low=0, high=7, shape=(1, 26), dtype=np.int8)\n",
    "        self.test = False\n",
    "        self.prev_answer = self.words[np.random.randint(0, len(self.words))]\n",
    "    # This resets the environment, chooses a new Wordle solution, and returns the initial game state.\n",
    "    def reset(self):\n",
    "        answer = self.words[np.random.randint(0, len(self.words))]\n",
    "        # Get a new Wordle solution with 0.1 probability if in training mode.\n",
    "        self.answer = answer if self.test else answer if np.random.random() < 0.1 else self.prev_answer\n",
    "        self.prev_answer = self.answer\n",
    "        self.state = np.zeros(26, dtype=np.int8)\n",
    "        self.counter = 0\n",
    "        self.success = set()\n",
    "        self.used = set()\n",
    "        return np.copy(self.state)\n",
    "    # This takes one step in the Wordle game with guess `action`, and returns the new state, reward, done status, and information.\n",
    "    def step(self, action):\n",
    "        reward = self._get_reward(action)\n",
    "        self._step(action)\n",
    "        done = self._get_done()\n",
    "        info = self._get_info()\n",
    "        return np.copy(self.state), reward, done, info\n",
    "    # If implemented, this hides the game board.\n",
    "    def close(self):\n",
    "        pass\n",
    "    # If implemented, this shows the game board.\n",
    "    def render(self):\n",
    "        pass\n",
    "    # This is a helper function for taking a step in the game.\n",
    "    def _step(self, action):\n",
    "        for i, c in enumerate(self.words[action]):\n",
    "            a = ord(c) - ord(\"a\")\n",
    "            if self.state[a] == 0:\n",
    "                self.state[a] = 2 if c in self.answer else 1\n",
    "            if self.state[a] == 2 and self.answer[i] == c:\n",
    "                self.state[a] = i + 3\n",
    "                self.success.add(i)\n",
    "        self.counter += 1\n",
    "        self.used.add(action)\n",
    "    # This calculates the reward given guess `action`.\n",
    "    def _get_reward(self, action):\n",
    "        word = self.words[action]\n",
    "        if word == self.answer:\n",
    "            return 1000\n",
    "        if word in self.used:\n",
    "            return -500\n",
    "        reward = -1000 if self.counter == 5 else 0\n",
    "        for i, c in enumerate(word):\n",
    "            a = ord(c) - ord(\"a\")\n",
    "            if self.state[a] >= 3:\n",
    "                reward += 100 if self.answer[i] == c else -100\n",
    "            elif self.state[a] == 1:\n",
    "                reward += -100\n",
    "            else:\n",
    "                reward += 100 if self.answer[i] == c else 50 if c in self.answer else 0\n",
    "        return reward\n",
    "    # This determines whether the game is over.\n",
    "    def _get_done(self):\n",
    "        return self.counter == 6 or len(self.success) == 5\n",
    "    # This returns metadata about the current game.\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"answer\": self.answer,\n",
    "            \"state\": self.state,\n",
    "            \"counter\": self.counter,\n",
    "            \"success\": self.success,\n",
    "            \"used\": self.used\n",
    "        }\n",
    "    # This sets the environment to testing.\n",
    "    def set_test(self):\n",
    "        self.test = True\n",
    "    # This sets the environment to training.\n",
    "    def set_explore(self):\n",
    "        self.test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "env = Wordle()\n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"q\", \"target_q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.load(\"q\", \"target_q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This simulates n games of Wordle with in testing mode.\n",
    "def simulate(agent, n):\n",
    "    agent.env.set_test()\n",
    "    total_success = 0\n",
    "    for _ in range(n):\n",
    "        data = []\n",
    "        state = agent.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.get_optimal_action(state)\n",
    "            state_, reward, done, info = agent.env.step(action)\n",
    "            data.append((state, agent.env.words[action], reward, state_, done, info))\n",
    "            state = state_\n",
    "            if len(info['success']) == 5:\n",
    "                total_success += 1\n",
    "    print(\"Accuracy: {0}\".format(total_success / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "simulate(agent, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f44983b097f6197d01c893818a3733e8fe39f961abb70113313d87ac307e19c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
